# Robots.txt for docs.edenia.com
# This file optimizes search engine crawling behavior for better SEO

# Allow all search engines to crawl the site
User-agent: *
Allow: /

# Disallow crawling of search pages and tag pages to avoid duplicate content
Disallow: /search
Disallow: /search/
Disallow: /tags/
Disallow: /*/tags/
Disallow: /404.html
Disallow: /404

# Block admin and development pages
Disallow: /admin/
Disallow: /_next/
Disallow: /api/
Disallow: /build/
Disallow: /node_modules/

# Block common Docusaurus internal paths
Disallow: /__docusaurus/
Disallow: /_static/
Disallow: /_next/
Disallow: /_webpack/

# Sitemap locations for better indexing
Sitemap: https://docs.edenia.com/sitemap.xml
Sitemap: https://docs.edenia.com/es/sitemap.xml

